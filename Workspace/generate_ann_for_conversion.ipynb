{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create an ANN for conversion\n",
    "\n",
    "in this notebook we create an ANN. Then train it as a robot. Then we create some validation data for the network. Then everything gets stored to disk, so it can be used in the next step to convert it to an snn with snn_toolbox\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create and train ann "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import errno\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "import ctypes\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "import gym, roboschool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.keras.__version__)\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "main_directory = \"/home/jovyan/base_repository/Workspace/ann_for_conversion/\".format(os.getpid())\n",
    "try:\n",
    "    mkdir_p(main_directory)\n",
    "except PermissionError:\n",
    "    print(\"The user running this notebook has no permission to create this folder. Please provide a path to a folder\"\n",
    "         + \" with write permissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = namedtuple('Config', [\n",
    "    'env_id',\n",
    "    'population_size',\n",
    "    'timesteps_per_gen',\n",
    "    'num_workers',\n",
    "    'learning_rate',\n",
    "    'noise_stdev',\n",
    "    'snapshot_freq',\n",
    "    'return_proc_mode',\n",
    "    'calc_obstat_prob',\n",
    "    'l2coeff',\n",
    "    'eval_prob'\n",
    "])\n",
    "\n",
    "Optimizations = namedtuple('Optimizations', [\n",
    "    'mirrored_sampling',\n",
    "    'fitness_shaping',\n",
    "    'weight_decay',\n",
    "    'discretize_actions',\n",
    "    'gradient_optimizer',\n",
    "    'observation_normalization',\n",
    "    'divide_by_stdev'\n",
    "])\n",
    "\n",
    "ModelStructure = namedtuple('ModelStructure', [\n",
    "    'ac_noise_std',\n",
    "    'ac_bins',\n",
    "    'hidden_dims',\n",
    "    'nonlin_type',\n",
    "    'optimizer',\n",
    "    'optimizer_args'\n",
    "])\n",
    "\n",
    "Task = namedtuple('Task', [\n",
    "    'theta', 'ob_mean', 'ob_std', 'task_id'])\n",
    "\n",
    "Result = namedtuple('Result', [\n",
    "    'noise_inds','returns', 'signreturns', 'lengths',\n",
    "    'eval_return', 'eval_length',\n",
    "    'ob_sum', 'ob_sumsq', 'ob_count',\n",
    "    'task_id',\n",
    "    'times_predict'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizations = Optimizations(\n",
    "    mirrored_sampling=True,\n",
    "    fitness_shaping=True,\n",
    "    weight_decay=True,\n",
    "    discretize_actions=False,\n",
    "    gradient_optimizer=False,\n",
    "    observation_normalization=True,\n",
    "    divide_by_stdev=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETURN_PROC_MODE_CR = 'centered_rank'\n",
    "RETURN_PROC_MODE_SIGN = 'sign'\n",
    "RETURN_PROC_MODE_CR_SIGN = 'centered_sign_rank'\n",
    "\n",
    "config = Config(\n",
    "    env_id=\"RoboschoolAnt-v1\",\n",
    "    population_size=10,\n",
    "    timesteps_per_gen=10000,\n",
    "    num_workers=os.cpu_count(),\n",
    "    learning_rate=0.001,\n",
    "    noise_stdev=0.02,\n",
    "    snapshot_freq=5,\n",
    "    return_proc_mode=RETURN_PROC_MODE_CR,\n",
    "    calc_obstat_prob=0.01,\n",
    "    l2coeff=0.005,\n",
    "    eval_prob=0.003\n",
    ")\n",
    "\n",
    "try:\n",
    "    env = gym.make(config.env_id)\n",
    "except:\n",
    "    print(\"Please provide a valid environment ID for the OpenAI Gym. {} is not valid.\".format(config.env_id))\n",
    "\n",
    "# These are used inside create_model for the input and output dimensions\n",
    "ob_space = env.observation_space\n",
    "ac_space = env.action_space\n",
    "\n",
    "assert config.population_size > 0\n",
    "assert config.num_workers > 0\n",
    "assert config.learning_rate > 0\n",
    "assert config.noise_stdev != 0\n",
    "assert config.eval_prob >= 0\n",
    "\n",
    "if (config.return_proc_mode != RETURN_PROC_MODE_CR\n",
    "    and config.return_proc_mode != RETURN_PROC_MODE_SIGN\n",
    "    and config.return_proc_mode != RETURN_PROC_MODE_CR_SIGN):\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    \n",
    "if optimizations.observation_normalization:\n",
    "    assert config.calc_obstat_prob > 0\n",
    "\n",
    "if optimizations.gradient_optimizer:\n",
    "    assert config.l2coeff > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER_ADAM = 'adam'\n",
    "OPTIMIZER_SGD = 'sgd'\n",
    "\n",
    "model_structure = ModelStructure(\n",
    "    ac_noise_std=0.01,\n",
    "    ac_bins=5,\n",
    "    hidden_dims=[256, 256],\n",
    "    nonlin_type='tanh',\n",
    "    optimizer=OPTIMIZER_ADAM,\n",
    "    optimizer_args={\n",
    "        'stepsize': config.learning_rate\n",
    "    }\n",
    ")\n",
    "\n",
    "assert model_structure.ac_noise_std >= 0\n",
    "assert isinstance(model_structure.hidden_dims, list)\n",
    "assert all(hd >= 0 for hd in model_structure.hidden_dims)\n",
    "\n",
    "if optimizations.gradient_optimizer:\n",
    "    if model_structure.optimizer != OPTIMIZER_ADAM and model_structure.optimizer != OPTIMIZER_SGD:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    try:\n",
    "        stepsize = model_structure.optimizer_args['stepsize']\n",
    "        assert stepsize > 0\n",
    "    except KeyError:\n",
    "        print(\"Please provide the stepsize parameter.\")\n",
    "\n",
    "if optimizations.discretize_actions:\n",
    "    assert model_structure.ac_bins > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_configuration(save_directory):\n",
    "    with open(os.path.join(save_directory, 'config.json'), 'w', encoding='utf-8') as f:\n",
    "        chained_dict = OrderedDict([\n",
    "            ('config', config._asdict()),\n",
    "            ('model_structure', model_structure._asdict()),\n",
    "            ('optimizations', optimizations._asdict())])\n",
    "\n",
    "        json.dump(chained_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(initial_weights=None, model_name=\"model\", ob_mean=None, ob_std=None):\n",
    "       \n",
    "    import tensorflow as tf\n",
    "    \n",
    "    class Normc_initializer(tf.keras.initializers.Initializer):\n",
    "        def __init__(self, std=1.0):\n",
    "            self.std=std\n",
    "\n",
    "        def __call__(self, shape, dtype=None, partition_info=None):\n",
    "            out = np.random.randn(*shape).astype(np.float32)\n",
    "            out *= self.std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "            return tf.constant(out)\n",
    "    \n",
    "    class ObservationNormalizationLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, ob_mean, ob_std, **kwargs):\n",
    "            self.ob_mean = ob_mean\n",
    "            self.ob_std = ob_std\n",
    "            super(ObservationNormalizationLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def call(self, x):\n",
    "            return tf.clip_by_value((x - self.ob_mean) / self.ob_std, -5.0, 5.0)\n",
    "        \n",
    "        # get_config and from_config need to implemented to be able to serialize the model\n",
    "        def get_config(self):\n",
    "            base_config = super(ObservationNormalizationLayer, self).get_config()\n",
    "            base_config['ob_mean'] = self.ob_mean\n",
    "            base_config['ob_std'] = self.ob_std\n",
    "            return base_config\n",
    "        \n",
    "        @classmethod\n",
    "        def from_config(cls, config):\n",
    "            return cls(**config)\n",
    "        \n",
    "    class DiscretizeActionsUniformLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, num_ac_bins, adim, ahigh, alow, **kwargs):\n",
    "            self.num_ac_bins = num_ac_bins\n",
    "            self.adim = adim\n",
    "            # ahigh, alow are NumPy arrays when extracting from the environment, but when the model is loaded from a h5\n",
    "            # File they get initialised as a normal list, where operations like subtraction does not work, thereforce\n",
    "            # cast them explicitly\n",
    "            self.ahigh = np.array(ahigh)\n",
    "            self.alow = np.array(alow)\n",
    "            super(DiscretizeActionsUniformLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def call(self, x):            \n",
    "            # Reshape to [n x i x j] where n is dynamically chosen, i equals action dimension and j equals the number\n",
    "            # of bins\n",
    "            scores_nab = tf.reshape(x, [-1, self.adim, self.num_ac_bins])\n",
    "            # This picks the bin with the greatest value\n",
    "            a = tf.argmax(scores_nab, 2)\n",
    "            \n",
    "            # Then transform the interval from [0, num_ac_bins - 1] to [-1, 1] which equals alow and ahigh\n",
    "            ac_range_1a = (self.ahigh - self.alow)[None, :]\n",
    "            return 1. / (self.num_ac_bins - 1.) * tf.keras.backend.cast(a, 'float32') * ac_range_1a + self.alow[None, :]        \n",
    "        \n",
    "        # get_config and from_config need to implemented to be able to serialize the model\n",
    "        def get_config(self):\n",
    "            base_config = super(DiscretizeActionsUniformLayer, self).get_config()\n",
    "            base_config['num_ac_bins'] = self.num_ac_bins\n",
    "            base_config['adim'] = self.adim\n",
    "            base_config['ahigh'] = self.ahigh\n",
    "            base_config['alow'] = self.alow\n",
    "            return base_config\n",
    "        \n",
    "        @classmethod\n",
    "        def from_config(cls, config):\n",
    "            return cls(**config)\n",
    "    \n",
    "    ac_space = env.action_space\n",
    "    ob_space = env.observation_space\n",
    "    \n",
    "    nonlin = tf.nn.tanh\n",
    "    \n",
    "    if model_structure.nonlin_type == 'relu':\n",
    "        nonlin = tf.nn.relu\n",
    "    elif model_structure.nonlin_type == 'lrelu':\n",
    "        nonlin = tf.nn.leaky_relu\n",
    "    elif model_structure.nonlin_type == 'elu':\n",
    "        nonlin = tf.nn.leaky_relu\n",
    "\n",
    "    # Policy network\n",
    "    input_layer = x = tf.keras.Input(ob_space.shape, dtype=tf.float32)\n",
    "    \n",
    "    if ob_mean is not None and ob_std is not None and optimizations.observation_normalization:\n",
    "        if ob_std.all() != 0:\n",
    "            x = ObservationNormalizationLayer(ob_mean, ob_std)(x)\n",
    "                \n",
    "    for hd in model_structure.hidden_dims:\n",
    "        x = tf.keras.layers.Dense(\n",
    "            hd, activation=nonlin,\n",
    "            kernel_initializer=Normc_initializer(std=1.0),\n",
    "            bias_initializer=tf.initializers.zeros())(x)\n",
    "\n",
    "    # Action dimension and the lowest and highest possible values for an action\n",
    "    adim, ahigh, alow = ac_space.shape[0], ac_space.high, ac_space.low        \n",
    "    \n",
    "    if optimizations.discretize_actions:\n",
    "        num_ac_bins = int(model_structure.ac_bins)\n",
    "        x = tf.keras.layers.Dense(\n",
    "                        adim * num_ac_bins,\n",
    "                        kernel_initializer=Normc_initializer(std=0.01),\n",
    "                        bias_initializer=tf.initializers.zeros())(x)\n",
    "        a = DiscretizeActionsUniformLayer(num_ac_bins, adim, ahigh, alow)(x)\n",
    "    else:\n",
    "        a = tf.keras.layers.Dense(\n",
    "            adim,\n",
    "            kernel_initializer=Normc_initializer(std=0.01),\n",
    "            bias_initializer=tf.initializers.zeros())(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=a, name=model_name)\n",
    "    \n",
    "    if initial_weights is not None:\n",
    "        set_from_flat(model, initial_weights)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(ob, model, random_stream=None):\n",
    "    time_predict_s = time.time()\n",
    "    action = model.predict_on_batch(ob)\n",
    "    time_predict_e = time.time() - time_predict_s\n",
    "\n",
    "    if random_stream is not None and model_structure.ac_noise_std != 0:\n",
    "        action += random_stream.randn(*action.shape) * model_structure.ac_noise_std\n",
    "    return action, time_predict_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningStat(object):\n",
    "    def __init__(self, shape, eps):\n",
    "        self.sum = np.zeros(shape, dtype=np.float32)\n",
    "        self.sumsq = np.full(shape, eps, dtype=np.float32)\n",
    "        self.count = eps\n",
    "\n",
    "    def increment(self, s, ssq, c):\n",
    "        self.sum += s\n",
    "        self.sumsq += ssq\n",
    "        self.count += c\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.sum / self.count\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.sqrt(np.maximum(self.sumsq / self.count - np.square(self.mean), 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_weights(ob_mean=None, ob_std=None):\n",
    "    \n",
    "    model = create_model(ob_mean=ob_mean, ob_std=ob_std)\n",
    "    \n",
    "    # Print out the model\n",
    "    model.summary()\n",
    "    \n",
    "    return model.get_weights()\n",
    "\n",
    "def initialize_parameter_vector():\n",
    "    with multiprocessing.Pool(1) as pool:\n",
    "        if optimizations.observation_normalization:\n",
    "            ob_stat = RunningStat(\n",
    "                env.observation_space.shape,\n",
    "                eps=1e-2  # eps to prevent dividing by zero at the beginning when computing mean/stdev\n",
    "                )\n",
    "            theta = pool.apply(func=get_initial_weights, args=(ob_stat.mean, ob_stat.std))\n",
    "        else:\n",
    "            theta = pool.apply(func=get_initial_weights)\n",
    "\n",
    "    return theta, sum(np.prod(v.shape) for v in theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, num_params):\n",
    "        self.dim = num_params\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, theta, globalg):\n",
    "        self.t += 1\n",
    "        step = self._compute_step(globalg)\n",
    "        ratio = np.linalg.norm(step) / np.linalg.norm(theta)\n",
    "        theta_new = theta + step\n",
    "        return theta_new, ratio\n",
    "\n",
    "    def _compute_step(self, globalg):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, num_params, stepsize, momentum=0.9):\n",
    "        Optimizer.__init__(self, num_params)\n",
    "        self.v = np.zeros(self.dim, dtype=np.float32)\n",
    "        self.stepsize, self.momentum = stepsize, momentum\n",
    "\n",
    "    def _compute_step(self, globalg):\n",
    "        self.v = self.momentum * self.v + (1. - self.momentum) * globalg\n",
    "        step = -self.stepsize * self.v\n",
    "        return step\n",
    "        \n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, num_params, stepsize, beta1=0.9, beta2=0.999, epsilon=1e-08):\n",
    "        Optimizer.__init__(self, num_params)\n",
    "        self.stepsize = stepsize\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = np.zeros(self.dim, dtype=np.float32)\n",
    "        self.v = np.zeros(self.dim, dtype=np.float32)\n",
    "\n",
    "    def _compute_step(self, globalg):\n",
    "        a = self.stepsize * np.sqrt(1 - self.beta2 ** self.t) / (1 - self.beta1 ** self.t)\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * globalg\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (globalg * globalg)\n",
    "        step = -a * self.m / (np.sqrt(self.v) + self.epsilon)\n",
    "        return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedNoiseTable(object):\n",
    "    def __init__(self, seed=123):\n",
    "        self.seed = seed\n",
    "        count = 250000\n",
    "        print('Sampling {} random numbers with seed {}'.format(count, self.seed))\n",
    "\n",
    "        # Instantiate an array of C float datatype with size count\n",
    "        self._shared_mem = multiprocessing.Array(ctypes.c_float, count)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        self.noise = np.ctypeslib.as_array(self._shared_mem.get_obj())\n",
    "        assert self.noise.dtype == np.float32\n",
    "        self.noise[:] = np.random.RandomState(seed).randn(count)\n",
    "        print('Sampled {} bytes'.format(self.noise.size * 4))\n",
    "\n",
    "    def get(self, i, dim):\n",
    "        return self.noise[i:i + dim]\n",
    "\n",
    "    def sample_index(self, stream, dim):\n",
    "        return stream.randint(0, len(self.noise) - dim + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat(theta):\n",
    "     return np.concatenate([np.reshape(v, [-1]) for v in theta], 0)\n",
    "\n",
    "def set_from_flat(model, theta):\n",
    "    old_theta = model.get_weights()\n",
    "    shapes = [v.shape for v in old_theta]\n",
    "    total_size = theta.size\n",
    "        \n",
    "    start = 0\n",
    "    reshapes = []\n",
    "    \n",
    "    for (shape, v) in zip(shapes, theta):\n",
    "        size = int(np.prod(shape))\n",
    "        reshapes.append(np.reshape(theta[start:start+size], shape))\n",
    "        start += size\n",
    "    \n",
    "    assert start == total_size\n",
    "    model.set_weights(reshapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, \n",
    "            model, \n",
    "            *, \n",
    "            render=False, \n",
    "            timestep_limit=None, \n",
    "            save_obs=False, \n",
    "            random_stream=None):\n",
    "    \"\"\"\n",
    "    If random_stream is provided, the rollout will take noisy actions with noise drawn from that stream.\n",
    "    Otherwise, no action noise will be added.\n",
    "    \"\"\"\n",
    "    \n",
    "    env_timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "    timestep_limit = env_timestep_limit if timestep_limit is None else min(timestep_limit, env_timestep_limit)\n",
    "    rews = []\n",
    "    times_predict = []\n",
    "    t = 0\n",
    "    if save_obs:\n",
    "        obs = []\n",
    "    ob = env.reset()\n",
    "    for _ in range(timestep_limit):\n",
    "        ac, time_predict = act(ob[None], model, random_stream=random_stream)\n",
    "        ac = ac[0]\n",
    "        times_predict.append(time_predict)\n",
    "        if save_obs:\n",
    "            obs.append(ob)\n",
    "        try:\n",
    "            ob, rew, done, _ = env.step(ac)\n",
    "        except AssertionError:\n",
    "            # Is thrown when for example ac is a list which has at least one entry with NaN\n",
    "            raise \n",
    "        rews.append(rew)\n",
    "        t += 1\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "    rews = np.array(rews, dtype=np.float32)\n",
    "    if save_obs:\n",
    "        return rews, t, np.array(obs), times_predict\n",
    "    return rews, t, times_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_and_update_ob_stat(env, model, rs, task_ob_stat):\n",
    "    if optimizations.observation_normalization and config.calc_obstat_prob != 0 and rs.rand() < config.calc_obstat_prob:\n",
    "        try:\n",
    "            rollout_rews, rollout_len, obs, times_predict = rollout(\n",
    "                env, model, save_obs=True, random_stream=rs)\n",
    "        except AssertionError:\n",
    "            raise\n",
    "        task_ob_stat.increment(obs.sum(axis=0), np.square(obs).sum(axis=0), len(obs))\n",
    "    else:\n",
    "        try:\n",
    "            rollout_rews, rollout_len, times_predict = rollout(env, model, random_stream=rs)\n",
    "        except AssertionError:\n",
    "            raise\n",
    "    return rollout_rews, rollout_len, times_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_worker(task_list, result_queue, stop_work, noise, num_params):\n",
    "    from tensorflow.keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    print(\"PID {}: Started worker\".format(os.getpid()))\n",
    "    \n",
    "    assert isinstance(noise, SharedNoiseTable)\n",
    "\n",
    "    # Setup\n",
    "    # Create a new gym environment object because each worker needs its own one\n",
    "    env = gym.make(config.env_id)\n",
    "        \n",
    "    # Random stream used for adding noise to the actions as well as deciding if the observation statistics shall be\n",
    "    # updated\n",
    "    rs = np.random.RandomState()\n",
    "    \n",
    "    wait_time = 1\n",
    "    \n",
    "    cached_task = None\n",
    "    cached_task_id = -1\n",
    "    model = None\n",
    "    \n",
    "    while not bool(stop_work.value):\n",
    "        # Get the latest Task from the Manger list\n",
    "        try:\n",
    "            task = task_list[-1]\n",
    "        except IndexError:\n",
    "            if wait_time > 100:\n",
    "                print(\"The task list does not get tasks, something went wrong in the Master. Aborting.\")\n",
    "                break\n",
    "            print(\"Task list is empty, waiting {} seconds before trying again\".format(wait_time))\n",
    "            wait_time *= 2\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "    \n",
    "        assert isinstance(task, Task)\n",
    "        task_id = task.task_id\n",
    "        assert isinstance(task_id, int)\n",
    "        \n",
    "        if task_id != cached_task_id:\n",
    "            cached_task = task\n",
    "            cached_task_id = task_id\n",
    "        \n",
    "            K.clear_session()\n",
    "            K.set_session(tf.Session(config=tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)))\n",
    "            model = create_model(initial_weights=cached_task.theta, \n",
    "                             model_name=str(os.getpid()),\n",
    "                             ob_mean=cached_task.ob_mean,\n",
    "                             ob_std=cached_task.ob_std)\n",
    "        \n",
    "        if rs.rand() < config.eval_prob:\n",
    "            # Evaluation sample\n",
    "            set_from_flat(model, cached_task.theta)\n",
    "            try:\n",
    "                eval_rews, eval_length, times_predict = rollout(env, model)\n",
    "            except AssertionError:\n",
    "                result_queue.put(None)\n",
    "                return\n",
    "            \n",
    "            result_queue.put(Result(\n",
    "                noise_inds=None,\n",
    "                returns=None,\n",
    "                signreturns=None,\n",
    "                lengths=None,\n",
    "                eval_return=eval_rews.sum(),\n",
    "                eval_length=eval_length,\n",
    "                ob_sum=None,\n",
    "                ob_sumsq=None,\n",
    "                ob_count=None,\n",
    "                task_id=cached_task_id,\n",
    "                times_predict=times_predict\n",
    "            ))\n",
    "            \n",
    "        else:\n",
    "            task_ob_stat = RunningStat(env.observation_space.shape, eps=0.)  # eps=0 because we're incrementing only\n",
    "            \n",
    "            noise_inds, returns, signreturns, lengths = [], [], [], []\n",
    "            times_predict = []\n",
    "            \n",
    "            while not noise_inds:\n",
    "\n",
    "                # Noise sample\n",
    "                noise_idx = noise.sample_index(rs, num_params)\n",
    "                \n",
    "                epsilon = config.noise_stdev * noise.get(noise_idx, num_params)\n",
    "                \n",
    "                # Evaluate the sampled noise\n",
    "                set_from_flat(model, cached_task.theta + epsilon)\n",
    "                \n",
    "                try:\n",
    "                    rews_pos, len_pos, times_predict_pos = rollout_and_update_ob_stat(env,\n",
    "                                                                                      model,\n",
    "                                                                                      rs=rs,\n",
    "                                                                                      task_ob_stat=task_ob_stat)\n",
    "                except AssertionError:\n",
    "                    result_queue.put(None)\n",
    "                    return\n",
    "                \n",
    "                # Gather results\n",
    "                noise_inds.append(noise_idx)\n",
    "                returns.append([rews_pos.sum()])\n",
    "                signreturns.append([np.sign(rews_pos).sum()])\n",
    "                lengths.append([len_pos])\n",
    "                \n",
    "                times_predict += times_predict_pos\n",
    "\n",
    "                # Mirrored sampling also evaluates the noise by subtracting it\n",
    "                if optimizations.mirrored_sampling:\n",
    "                    set_from_flat(model, cached_task.theta - epsilon)\n",
    "                    \n",
    "                    try:\n",
    "                        rews_neg, len_neg, times_predict_neg = rollout_and_update_ob_stat(env,\n",
    "                                                                                          model, \n",
    "                                                                                          rs=rs, \n",
    "                                                                                          task_ob_stat=task_ob_stat)  \n",
    "                    except AssertionError:\n",
    "                        result_queue.put(None)\n",
    "                        return\n",
    "\n",
    "                    returns[-1].append(rews_neg.sum())\n",
    "                    signreturns[-1].append(np.sign(rews_neg).sum())\n",
    "                    lengths[-1].append(len_neg)\n",
    "                    \n",
    "                    times_predict += times_predict_neg\n",
    "\n",
    "            \n",
    "            result_queue.put(Result(\n",
    "                noise_inds=np.array(noise_inds),\n",
    "                returns=np.array(returns, dtype=np.float32),\n",
    "                signreturns=np.array(signreturns, dtype=np.float32),\n",
    "                lengths=np.array(lengths, dtype=np.int32),\n",
    "                eval_return=None,\n",
    "                eval_length=None,\n",
    "                ob_sum=None if task_ob_stat.count == 0 else task_ob_stat.sum,\n",
    "                ob_sumsq=None if task_ob_stat.count == 0 else task_ob_stat.sumsq,\n",
    "                ob_count=task_ob_stat.count,\n",
    "                task_id=cached_task_id,\n",
    "                times_predict=times_predict\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itergroups(items, group_size):\n",
    "    assert group_size >= 1\n",
    "    group = []\n",
    "    for x in items:\n",
    "        group.append(x)\n",
    "        if len(group) == group_size:\n",
    "            yield tuple(group)\n",
    "            del group[:]\n",
    "    if group:\n",
    "        yield tuple(group)\n",
    "        \n",
    "def batched_weighted_sum(weights, vecs, batch_size):\n",
    "    total = 0.\n",
    "    num_items_summed = 0\n",
    "    for batch_weights, batch_vecs in zip(itergroups(weights, batch_size), itergroups(vecs, batch_size)):\n",
    "        assert len(batch_weights) == len(batch_vecs) <= batch_size\n",
    "        total += np.dot(np.asarray(batch_weights, dtype=np.float32), np.asarray(batch_vecs, dtype=np.float32))\n",
    "        num_items_summed += len(batch_weights)\n",
    "    return total, num_items_summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranks(x):\n",
    "    \"\"\"\n",
    "    Returns ranks in [0, len(x))\n",
    "    Note: This is different from scipy.stats.rankdata, which returns ranks in [1, len(x)].\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1\n",
    "    ranks = np.empty(len(x), dtype=int)\n",
    "    ranks[x.argsort()] = np.arange(len(x))\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def compute_centered_ranks(x):\n",
    "    y = compute_ranks(x.ravel()).reshape(x.shape).astype(np.float32)\n",
    "    y /= (x.size - 1)\n",
    "    y -= .5\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(save_directory, stop_work, save_tasks_queue):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import backend as K\n",
    "    print(\"PID {}: Started saving process\".format(os.getpid()))\n",
    "    while not bool(stop_work.value):\n",
    "        save_task = save_tasks_queue.get()\n",
    "        assert isinstance(save_task, Task)\n",
    "        # We are creating models in a loop therefore we need to clear the session to avoid build up\n",
    "        K.clear_session()\n",
    "        model = create_model(initial_weights=save_task.theta, \n",
    "                            model_name=config.env_id + \"_Generation_\" + str(save_task.task_id),\n",
    "                            ob_mean=save_task.ob_mean,\n",
    "                            ob_std=save_task.ob_std)\n",
    "        \n",
    "        model.save(os.path.join(save_directory, \"snapshot_{:05d}.h5\".format(save_task.task_id)))\n",
    "        save_tasks_queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_master(max_timesteps=np.inf, seed=123):\n",
    "    save_directory = os.path.join(main_directory, time.strftime(\"%Y%m%dT%H%M%S\", time.localtime(time.time())))\n",
    "    mkdir_p(save_directory)\n",
    "    save_configuration(save_directory)\n",
    "\n",
    "    rs = np.random.RandomState()\n",
    "\n",
    "    noise = SharedNoiseTable(seed)\n",
    "\n",
    "    theta, num_params = initialize_parameter_vector()\n",
    "    theta = get_flat(theta)\n",
    "\n",
    "    if optimizations.gradient_optimizer:\n",
    "        if model_structure.optimizer == OPTIMIZER_ADAM:\n",
    "            optimizer = Adam(int(num_params), **model_structure.optimizer_args)\n",
    "        elif model_structure.optimizer == OPTIMIZER_SGD:\n",
    "            optimizer = SGD(int(num_params), **model_structure.optimizer_args)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    manager = multiprocessing.Manager()\n",
    "    task_list = manager.list()\n",
    "    result_queue = multiprocessing.Queue()\n",
    "    save_tasks_queue = multiprocessing.JoinableQueue()\n",
    "    stop_work = multiprocessing.Value('i', 0, lock=False)\n",
    "\n",
    "    # Start workers\n",
    "    workers = []\n",
    "    for _ in range(config.num_workers):\n",
    "        worker = multiprocessing.Process(target=run_worker, args=(task_list, result_queue, stop_work, noise, num_params))\n",
    "        workers.append(worker)\n",
    "        worker.start()\n",
    "        \n",
    "    save_process = multiprocessing.Process(target=save_model, args=(save_directory, stop_work, save_tasks_queue))\n",
    "    save_process.start()\n",
    "\n",
    "    episodes_so_far = 0\n",
    "    timesteps_so_far = 0\n",
    "    generations = 0\n",
    "    tstart = time.time()\n",
    "\n",
    "    # Only used with observation_normalization optimization\n",
    "    ob_stat = RunningStat(\n",
    "        env.observation_space.shape,\n",
    "        eps=1e-2  # eps to prevent dividing by zero at the beginning when computing mean/stdev\n",
    "        )\n",
    "\n",
    "    generation_log = OrderedDict()\n",
    "    generation_log_file = os.path.join(save_directory, 'log.csv')\n",
    "    fieldnames = [\n",
    "        'Generation',\n",
    "        'GenRewMean', 'GenRewStd', 'GenLenMean', \n",
    "        'EvalGenRewardMean', 'EvalGenRewardStd', 'EvalGenLengthMean', 'EvalGenCount',\n",
    "        'EpisodesThisGen', 'EpisodesSoFar', 'TimestepsThisGen', 'TimestepsSoFar',\n",
    "        'UniqueWorkers', 'ResultsSkippedFrac', 'ObCount',\n",
    "        'TimeElapsedThisGen', 'TimeElapsed',\n",
    "        'TimePredictMin', 'TimePredictMax', 'TimePredictMean', 'TimePredictCount']\n",
    "\n",
    "    with open(generation_log_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "    while timesteps_so_far < max_timesteps:\n",
    "        step_tstart = time.time()\n",
    "\n",
    "        task_list.append(Task(\n",
    "            theta=theta,\n",
    "            ob_mean=ob_stat.mean if optimizations.observation_normalization else None,\n",
    "            ob_std=ob_stat.std if optimizations.observation_normalization else None,\n",
    "            task_id=generations\n",
    "        ))\n",
    "\n",
    "        print(\"---------------- Generation: {}----------------\".format(generations))\n",
    "\n",
    "        assert theta.dtype == np.float32\n",
    "\n",
    "        curr_task_results, eval_returns, eval_lengths = [], [], []\n",
    "        num_results_skipped, num_episodes_popped, num_timesteps_popped, ob_count_this_gen = 0, 0, 0, 0\n",
    "\n",
    "        times_predict = []\n",
    "        \n",
    "        stop_training = False\n",
    "\n",
    "        print(\"PID {}: Waiting for results\".format(os.getpid()))\n",
    "\n",
    "        while num_episodes_popped < config.population_size or num_timesteps_popped < config.timesteps_per_gen:\n",
    "            result = result_queue.get()\n",
    "            \n",
    "            if result is None:\n",
    "                print(\"Stopping training. The model produced non finite numbers inside the action vector. Try a\"\n",
    "                      + \" different configuration.\")\n",
    "                stop_training = True\n",
    "                break\n",
    "            \n",
    "            assert isinstance(result, Result)\n",
    "            task_id = result.task_id\n",
    "            assert isinstance(task_id, int)\n",
    "\n",
    "            assert (result.eval_return is None) == (result.eval_length is None)\n",
    "\n",
    "            if result.eval_length is not None:\n",
    "                # The result was an evaluation job therefore do not collect the result only the evaluation\n",
    "                if task_id == generations:\n",
    "                    eval_returns.append(result.eval_return)\n",
    "                    eval_lengths.append(result.eval_length)\n",
    "                    times_predict += result.times_predict\n",
    "            else:\n",
    "                assert result.noise_inds.ndim == 1 and result.returns.dtype == np.float32\n",
    "\n",
    "                if optimizations.mirrored_sampling:\n",
    "                    assert result.returns.shape == result.lengths.shape == (len(result.noise_inds), 2)\n",
    "                else:\n",
    "                    assert result.returns.shape == result.lengths.shape == (len(result.noise_inds), 1)\n",
    "\n",
    "                if task_id == generations:\n",
    "                    curr_task_results.append(result)\n",
    "                                    \n",
    "                    # Update counts\n",
    "                    result_num_eps = result.lengths.size\n",
    "                    result_num_timesteps = result.lengths.sum()\n",
    "                    episodes_so_far += result_num_eps\n",
    "                    timesteps_so_far += result_num_timesteps\n",
    "                    \n",
    "                    num_episodes_popped += result_num_eps\n",
    "                    num_timesteps_popped += result_num_timesteps\n",
    "\n",
    "                    # Update observation stats if the optimization is used\n",
    "                    if optimizations.observation_normalization and result.ob_count > 0:\n",
    "                        ob_stat.increment(result.ob_sum, result.ob_sumsq, result.ob_count)\n",
    "                        ob_count_this_gen += result.ob_count\n",
    "                        \n",
    "                    times_predict += result.times_predict\n",
    "                else:\n",
    "                    num_results_skipped += 1\n",
    "\n",
    "        if stop_training:\n",
    "            break\n",
    "            \n",
    "        print(\"Gathered results\")\n",
    "\n",
    "        # Compute skip fraction\n",
    "        frac_results_skipped = num_results_skipped / (num_results_skipped + len(curr_task_results))\n",
    "        if num_results_skipped > 0:\n",
    "            print(\"Skipped {} out of date results ({:.2f}%)\".format(\n",
    "                num_results_skipped, 100. * frac_results_skipped))\n",
    "\n",
    "        # Assemble results\n",
    "        noise_inds = np.concatenate([r.noise_inds for r in curr_task_results])\n",
    "        returns = np.concatenate([r.returns for r in curr_task_results])\n",
    "        lengths = np.concatenate([r.lengths for r in curr_task_results])\n",
    "        assert noise_inds.shape[0] == returns.shape[0] == lengths.shape[0]\n",
    "\n",
    "        # If fitness shaping is turned on rank the results\n",
    "        if optimizations.fitness_shaping:\n",
    "            if config.return_proc_mode == RETURN_PROC_MODE_CR:\n",
    "                proc_returns = compute_centered_ranks(returns)\n",
    "            # sign and centered_sign_rank are obviously only useful in combination with mirrored sampling\n",
    "            elif config.return_proc_mode == RETURN_PROC_MODE_SIGN:\n",
    "                proc_returns = np.concatenate([r.signreturns for r in curr_task_results])\n",
    "            elif config.return_proc_mode == RETURN_PROC_MODE_CR_SIGN:\n",
    "                proc_returns = compute_centered_ranks(np.concatenate([r.signreturns for r in curr_task_results]))\n",
    "            else:\n",
    "                # Throw error to indicate the false input instead of silently pass on.\n",
    "                # This should have been already catched in the configuration section, so this here is a misconfiguration.\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            proc_returns = returns\n",
    "\n",
    "        # Mirrored sampling returns a 2D numpy array therefore we need to preprocess it accordingly\n",
    "        if optimizations.mirrored_sampling:\n",
    "            # Calculates the difference between the rewards sampled with the positive and negative noise\n",
    "            proc_returns = proc_returns[:, 0] - proc_returns[:, 1]\n",
    "        else:\n",
    "            proc_returns = proc_returns.ravel()\n",
    "\n",
    "        # Calculate the approximated gradient with a batch variant which saves time on large vectors\n",
    "        g, count = batched_weighted_sum(\n",
    "            proc_returns,\n",
    "            (noise.get(idx, num_params) for idx in noise_inds),\n",
    "            batch_size=500\n",
    "        )\n",
    "\n",
    "        assert g.shape == (num_params,) and g.dtype == np.float32 and count == len(noise_inds)\n",
    "        \n",
    "        # Update with the approximated gradient\n",
    "        g /= returns.size\n",
    "        \n",
    "        if optimizations.divide_by_stdev:\n",
    "            g /= config.noise_stdev\n",
    "            \n",
    "        if optimizations.gradient_optimizer:\n",
    "            step = -g\n",
    "            if optimizations.weight_decay:\n",
    "                step += config.l2coeff * theta\n",
    "            theta, _ = optimizer.update(theta, step)\n",
    "        else:\n",
    "            step = g * config.learning_rate\n",
    "            if optimizations.weight_decay:\n",
    "                step *= config.l2coeff\n",
    "            theta += step\n",
    "        \n",
    "        step_tend = time.time()\n",
    "\n",
    "        # Log the generation and print to stdout\n",
    "        generation_log['Generation'] = generations\n",
    "\n",
    "        generation_log['GenRewMean'] = returns.mean()\n",
    "        generation_log['GenRewStd'] = returns.std()\n",
    "        generation_log['GenLenMean'] = lengths.mean()\n",
    "\n",
    "        generation_log['EvalGenRewardMean'] = np.nan if not eval_returns else np.mean(eval_returns)\n",
    "        generation_log['EvalGenRewardStd'] = np.nan if not eval_returns else np.std(eval_returns)\n",
    "        generation_log['EvalGenLengthMean'] = np.nan if not eval_lengths else np.mean(eval_lengths)\n",
    "        generation_log['EvalGenCount'] = len(eval_returns)\n",
    "\n",
    "        generation_log['EpisodesThisGen'] = lengths.size\n",
    "        generation_log['EpisodesSoFar'] = episodes_so_far\n",
    "        generation_log['TimestepsThisGen'] = lengths.sum()\n",
    "        generation_log['TimestepsSoFar'] = timesteps_so_far\n",
    "\n",
    "        generation_log['UniqueWorkers'] = config.num_workers\n",
    "        generation_log['ResultsSkippedFrac'] = frac_results_skipped\n",
    "        generation_log['ObCount'] = ob_count_this_gen\n",
    "\n",
    "        generation_log['TimeElapsedThisGen'] = step_tend - step_tstart\n",
    "        generation_log['TimeElapsed'] = step_tend - tstart\n",
    "\n",
    "        generation_log['TimePredictMin'] = np.amin(times_predict)\n",
    "        generation_log['TimePredictMax'] = np.amax(times_predict)\n",
    "        generation_log['TimePredictMean'] = np.mean(times_predict)\n",
    "        generation_log['TimePredictCount'] = len(times_predict)\n",
    "\n",
    "        for key, value in generation_log.items():\n",
    "            print(f'{key:25} {value}')\n",
    "\n",
    "        # Append the log the csv file\n",
    "        with open(generation_log_file, 'a', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writerow(generation_log)\n",
    "\n",
    "        # Note that the model is created with a custom layer and custom initializer, and therefore needs these two\n",
    "        # custom classes if one wants to load a saved model\n",
    "        if config.snapshot_freq != 0 and generations % config.snapshot_freq == 0:\n",
    "            \n",
    "            save_tasks_queue.put(Task(\n",
    "                theta=theta,\n",
    "                ob_mean=ob_stat.mean if optimizations.observation_normalization else None,\n",
    "                ob_std=ob_stat.std if optimizations.observation_normalization else None,\n",
    "                task_id=generations\n",
    "                ))\n",
    "\n",
    "            print(\"Saved model in generation {} to {}\".format(generations, save_directory))\n",
    "\n",
    "        generations += 1\n",
    "\n",
    "    # Quit the multiprocessing data structures and processes\n",
    "    stop_work.value = 1\n",
    "\n",
    "    result_queue.close()\n",
    "    \n",
    "    for w in workers:\n",
    "        # Workers are blocking on empty queues and cannot be joined. When attempted they will try to join forever\n",
    "        # Therefore we terminate the process. This is not crucial since we already saved everything for the last\n",
    "        # generation.\n",
    "        w.terminate()\n",
    "        \n",
    "    # Save tasks queue is a joinable queue, therefore we can gracefully join the queue\n",
    "    save_tasks_queue.join()\n",
    "    save_tasks_queue.close()\n",
    "\n",
    "    # Like the worker processes a join would result in an indefinite block, since save_tasks_queue is closed and all\n",
    "    # save jobs have been processed we can terminate the process\n",
    "    save_process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_master(max_timesteps=170000, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate validation data\n",
    "\n",
    "Now we the ANN are done. But we also need some validation data to see if the final SNN works as expected. \n",
    "\n",
    "the following cells read the latest ANN from disc, then let it run once while observing it's exact inputs and outputs. These data will then also be stored on discs right next to the ANN itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import roboschool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gym import wrappers\n",
    "from ipywidgets import Video\n",
    "import ipywidgets as widgets\n",
    "from multiprocessing import Pool, Process\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load load-model.py\n",
    "def load_model(model_path):   \n",
    "    import tensorflow as tf\n",
    "\n",
    "    class Normc_initializer(tf.keras.initializers.Initializer):\n",
    "        def __init__(self, std=1.0):\n",
    "            self.std=std\n",
    "\n",
    "        def __call__(self, shape, dtype=None, partition_info=None):\n",
    "            out = np.random.randn(*shape).astype(np.float32)\n",
    "            out *= self.std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "            return tf.constant(out)\n",
    "    \n",
    "    class ObservationNormalizationLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, ob_mean, ob_std, **kwargs):\n",
    "            self.ob_mean = ob_mean\n",
    "            self.ob_std = ob_std\n",
    "            super(ObservationNormalizationLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def call(self, x):\n",
    "            return tf.clip_by_value((x - self.ob_mean) / self.ob_std, -5.0, 5.0)\n",
    "        \n",
    "        # get_config and from_config need to implemented to be able to serialize the model\n",
    "        def get_config(self):\n",
    "            base_config = super(ObservationNormalizationLayer, self).get_config()\n",
    "            base_config['ob_mean'] = self.ob_mean\n",
    "            base_config['ob_std'] = self.ob_std\n",
    "            return base_config\n",
    "        \n",
    "        @classmethod\n",
    "        def from_config(cls, config):\n",
    "            return cls(**config)\n",
    "        \n",
    "    class DiscretizeActionsUniformLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, num_ac_bins, adim, ahigh, alow, **kwargs):\n",
    "            self.num_ac_bins = num_ac_bins\n",
    "            self.adim = adim\n",
    "            # ahigh, alow are NumPy arrays when extracting from the environment, but when the model is loaded from a h5\n",
    "            # File they get initialised as a normal list, where operations like subtraction does not work, thereforce\n",
    "            # cast them explicitly\n",
    "            self.ahigh = np.array(ahigh)\n",
    "            self.alow = np.array(alow)\n",
    "            super(DiscretizeActionsUniformLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def call(self, x):            \n",
    "            # Reshape to [n x i x j] where n is dynamically chosen, i equals action dimension and j equals the number\n",
    "            # of bins\n",
    "            scores_nab = tf.reshape(x, [-1, self.adim, self.num_ac_bins])\n",
    "            # This picks the bin with the greatest value\n",
    "            a = tf.argmax(scores_nab, 2)\n",
    "            \n",
    "            # Then transform the interval from [0, num_ac_bins - 1] to [-1, 1] which equals alow and ahigh\n",
    "            ac_range_1a = (self.ahigh - self.alow)[None, :]\n",
    "            return 1. / (self.num_ac_bins - 1.) * tf.keras.backend.cast(a, 'float32') * ac_range_1a + self.alow[None, :]        \n",
    "        \n",
    "        # get_config and from_config need to implemented to be able to serialize the model\n",
    "        def get_config(self):\n",
    "            base_config = super(DiscretizeActionsUniformLayer, self).get_config()\n",
    "            base_config['num_ac_bins'] = self.num_ac_bins\n",
    "            base_config['adim'] = self.adim\n",
    "            base_config['ahigh'] = self.ahigh\n",
    "            base_config['alow'] = self.alow\n",
    "            return base_config\n",
    "        \n",
    "        @classmethod\n",
    "        def from_config(cls, config):\n",
    "            return cls(**config)\n",
    "    \n",
    "    custom_objects = {'Normc_initializer' : Normc_initializer, \n",
    "                      'ObservationNormalizationLayer' : ObservationNormalizationLayer,\n",
    "                      'DiscretizeActionsUniformLayer' : DiscretizeActionsUniformLayer}\n",
    "    \n",
    "    try:\n",
    "        model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_evaluation(env, model, render=False, timestep_limit=None, random_stream=None):\n",
    "    \"\"\"\n",
    "    If random_stream is provided, the rollout will take noisy actions with noise drawn from that stream.\n",
    "    Otherwise, no action noise will be added.\n",
    "    \"\"\"\n",
    "\n",
    "    env_timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "    timestep_limit = env_timestep_limit if timestep_limit is None else min(timestep_limit, env_timestep_limit)\n",
    "    rews = []\n",
    "    t = 0\n",
    "\n",
    "    ob = env.reset()\n",
    "    inputs=[]\n",
    "    results=[]\n",
    "    for _ in range(timestep_limit):\n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        inputs.append(ob[None])\n",
    "        x = model.predict_on_batch(ob[None])\n",
    "        results.append(x)\n",
    "        ac = x[0]\n",
    "        try:\n",
    "            ob, rew, done, _ = env.step(ac)\n",
    "        except AssertionError:\n",
    "            # Is thrown when for example ac is a list which has at least one entry with NaN\n",
    "            raise \n",
    "        rews.append(rew)\n",
    "        t += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    x_test = np.concatenate(inputs)\n",
    "    y_test = np.concatenate(results)\n",
    "    np.savez_compressed(os.path.join(save_directory,'x_test'), x_test)\n",
    "    np.savez_compressed(os.path.join(save_directory,'y_test'), y_test)\n",
    "\n",
    "    return np.array(rews, dtype=np.float32), t\n",
    "\n",
    "\n",
    "def run_model(model_file_path, model_file, save_directory, record=False):   \n",
    "    \n",
    "        with open(os.path.join(model_file_path, \"config.json\"), encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "    \n",
    "        env = gym.make(config['config']['env_id'])\n",
    "        env.reset()\n",
    "\n",
    "        if record:\n",
    "            env = wrappers.Monitor(env, save_directory, force=True)\n",
    "\n",
    "        model = load_model(os.path.join(model_file_path, model_file))\n",
    "\n",
    "        try:\n",
    "            rewards, length = rollout_evaluation(env, model)\n",
    "        except AssertionError:\n",
    "            print(\"The model file provided produces non finite numbers. Stopping.\")\n",
    "            return\n",
    "        \n",
    "        print(rewards)\n",
    "        print([rewards.sum(), length])\n",
    "\n",
    "        return [rewards.sum(), length]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "#ann_path='/home/jovyan/work/evolution-strategies/training_runs/26_11_2019-11h_14m_11s/snapshot_00054.h5'\n",
    "#ann_name=\"example_ann\"\n",
    "#config='/home/jovyan/work/evolution-strategies/training_runs/26_11_2019-11h_14m_11s/config.json'\n",
    "\n",
    "# evolution-strategiestraining_runs\n",
    "list_of_folders = glob.glob(main_directory+'/*') # * means all if need specific format then *.csv\n",
    "latest_folder = max(list_of_folders, key=os.path.getctime)\n",
    "print(latest_folder)\n",
    "list_of_files = glob.glob(latest_folder+\"/snapshot_*.h5\") # * means all if need specific format then *.csv\n",
    "latest_file = max(list_of_files, key=os.path.getctime)\n",
    "print(latest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_file_path = \"/home/jovyan/work/evolution-strategies/training_runs/12_11_2019-13h_08m_06s\"\n",
    "model_file_path = latest_folder\n",
    "#model_file_name = \"snapshot_00013.h5\"\n",
    "#model_file_path = \"/home/jovyan/base_repository/Workspace/ann_training_run/\"\n",
    "import ntpath\n",
    "model_file_name = ntpath.basename(latest_file)\n",
    "\n",
    "# Lets store the video file in the same directory as the model file\n",
    "save_directory = model_file_path\n",
    "\n",
    "\n",
    "#with Pool(os.cpu_count()) as pool:\n",
    "#    pool.apply(func=run_model, args=(model_file_path, model_file_name, save_directory, True))\n",
    "    \n",
    "run_model(model_file_path, model_file_name, save_directory, True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
